{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# TODO\n",
    "1. Create and pass tests for fit and predict (including making sure our random seed thing is working actually)\n",
    "2. change technical report calling code to use a stratified random subsampling method (instead of k-fold) to accomodate for large dataset\n",
    "3. make visualizations to potentially eliminate attribute\n",
    "4. make visualization of classifications to potentially include \"distinction\" as a subset of pass or withdraw as a subset of fail \n",
    "5. make whole API thing\n",
    "6. make and prepare for Final presentation (five min)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Introduction\n",
    "\n",
    "From our dataset, we will be using the attributes gender, region, highest_education, age_band, num_of_prev_attempts, studied_credits, imd_band$^1$, disability to predict the classifier final_result.\n",
    "\n",
    "## Cleaning our data\n",
    "\n",
    "###  Data Analysis\n",
    "Todo:\n",
    "* Frequency Bar Charts \n",
    "* Relevant summary statistics about the dataset.\n",
    "\n",
    "\n",
    "Because studied_credits, and num_of_prev_attempts are numerical attributes. We will convert them to categorical. The table below denotes how studied_credits will be converted, and num_of_prev_attempts will be converted to a bool where True denotes that the class has been previously attempted, and False denotes that it has not. \n",
    "\n",
    "|number|credit ranges|\n",
    "|------|-------------|\n",
    "|1|$\\leq$ 59|\n",
    "|2|60-119|\n",
    "|3|120-179|\n",
    "|4|180-239|\n",
    "|5|$\\geq$ 240 |\n",
    "\n",
    "We further cleaned our data by dropping rows with missing values. We understand that this could impact the affects of our data as we would me losing out on some entries. However, upon further inspection we came to realize that only about 1,000 of the over 30,000 rows of data contained missing values. This small ratio made us sure that losing out on this data would not have a negative impact on our classifier. \n",
    "\n",
    "1: imd_band is a measure of poverty based on area in the UK.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "from tabulate import tabulate\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MySimpleLinearRegressor, MyNaiveBayesClassifier, MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "student_data = MyPyTable().load_from_file(\"input_data/studentInfo.csv\")\n",
    "# remove missing values\n",
    "student_data.remove_rows_with_missing_values()\n",
    "\n",
    "gender = student_data.get_column(\"gender\")\n",
    "region = student_data.get_column(\"region\")\n",
    "highest_education = student_data.get_column(\"highest_education\") \n",
    "age_band = student_data.get_column(\"age_band\")\n",
    "num_of_prev_attempts = student_data.get_column(\"num_of_prev_attempts\") \n",
    "studied_credits = student_data.get_column(\"studied_credits\")\n",
    "\n",
    "myutils.convert_vals_into_cutoffs(num_of_prev_attempts, [0, 1], [False, True])\n",
    "myutils.convert_vals_into_cutoffs(studied_credits, [59, 60, 120, 180, 240,], [1,2,3,4,5])\n",
    "\n",
    "for i in range(len(student_data.data)):\n",
    "    student_data.data[i][8] = num_of_prev_attempts[i]\n",
    "    student_data.data[i][9] = studied_credits[i]\n",
    "\n",
    "imd_band = student_data.get_column(\"imd_band\")\n",
    "disability = student_data.get_column(\"disability\")\n",
    "final_result = student_data.get_column(\"final_result\")"
   ]
  },
  {
   "source": [
    "## Classifications\n",
    "\n",
    "### Accuracy Rates\n",
    "For the purposes of this demo, we ran our, now clean, data through our decisions tree classifiers and naive bayes classifier. Naive Bayes is a bit more accurate than decision tree and our accuracy was a bit disappointing. We are hoping that our RandomForestClassifier will solve some of the overfitting and improve our accuracy rate for our final project.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Stratified 10-Fold Cross Validation\n",
      "Naive Bayes: accuracy =  0.4282447112635792 error rate =  0.5717552887364208\n",
      "Tree: accuracy =  0.3956864239883108 error rate =  0.6043135760116892\n"
     ]
    }
   ],
   "source": [
    "student_train_folds, student_test_folds = myevaluation.stratified_kfold_cross_validation(student_data.data, final_result, 10) \n",
    "\n",
    "student_test = []\n",
    "student_train = []\n",
    "\n",
    "final_results_test = []\n",
    "final_results_train = []\n",
    "\n",
    "# turn indexes into data sets\n",
    "for row in student_train_folds:\n",
    "    student_set = []\n",
    "    final_results_set = []\n",
    "    for item in row:\n",
    "        student_set.append(student_data.data[item][3:10])\n",
    "        final_results_set.append(student_data.data[item][-1])\n",
    "    student_train.append(student_set)\n",
    "    final_results_train.append(final_results_set)\n",
    "\n",
    "# turn indexes into data sets\n",
    "for row in student_test_folds:\n",
    "    student_set = []\n",
    "    final_results_set = []\n",
    "    for item in row:\n",
    "        student_set.append(student_data.data[item][3:10])\n",
    "        final_results_set.append(student_data.data[item][-1])\n",
    "    student_test.append(student_set)\n",
    "    final_results_test.append(final_results_set)\n",
    "\n",
    "#Naive Bayes model\n",
    "total_Naive = []\n",
    "total_expected = []\n",
    "for i in range(10):\n",
    "    student_Naive = MyNaiveBayesClassifier()    \n",
    "    student_Naive.fit(student_train[i],final_results_train[i])\n",
    "    Naive_predictions = student_Naive.predict(student_test[i])\n",
    "    total_Naive.extend(Naive_predictions)\n",
    "    total_expected.extend(final_results_test[i])\n",
    "print(\"===========================================\")\n",
    "print(\"Predictive Accuracy\")\n",
    "print(\"===========================================\")\n",
    "print(\"Stratified 10-Fold Cross Validation\")\n",
    "accuracy, errorrate = myutils.accuracy_errorrate(total_Naive, total_expected)\n",
    "print(\"Naive Bayes: accuracy = \", accuracy, \"error rate = \", errorrate)\n",
    "\n",
    "#Tree model\n",
    "total_tree = []\n",
    "total_expected = []\n",
    "for i in range(10):\n",
    "    student_tree = MyDecisionTreeClassifier()    \n",
    "    student_tree.fit(student_train[i],final_results_train[i])\n",
    "    tree_predictions = student_tree.predict(student_test[i])\n",
    "    total_tree.extend(tree_predictions)\n",
    "    total_expected.extend(final_results_test[i])\n",
    "\n",
    "\n",
    "\n",
    "accuracy, errorrate = myutils.accuracy_errorrate(total_tree, total_expected)\n",
    "print(\"Tree: accuracy = \", accuracy, \"error rate = \", errorrate)"
   ]
  },
  {
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "We then made a confusion matrix for our decision tree classifier to see how our predicitions were distributed. It shows how the predictions are not clustered along the diagonal and this is due to the classifier's inaccuracies. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nDecision Tree (Stratified 10 Fold Cross Validation Results)\nFinal Result      Pass    Withdrawn    Fail    Distinction    total    Recognition %\n--------------  ------  -----------  ------  -------------  -------  ---------------\nPass              7496         3066    1088            180    11830         63.3643\nWithdrawn         4861         4014     936            109     9920         40.4637\nFail              3721         2253     872             61     6907         12.6249\nDistinction       2017          544     189             75     2825          2.65487\n"
     ]
    }
   ],
   "source": [
    "tree_matrix = myevaluation.confusion_matrix(total_expected, total_tree, [\"Pass\",\"Withdrawn\", \"Fail\", \"Distinction\"])\n",
    "\n",
    "for i in range(len(tree_matrix)):\n",
    "    total = 0\n",
    "    rec = 0\n",
    "    for item in tree_matrix[i]:\n",
    "        total += item\n",
    "    true_pos = tree_matrix[i][i]\n",
    "    if total != 0:\n",
    "        rec = (true_pos/total)*100\n",
    "    tree_matrix[i].append(total)\n",
    "    tree_matrix[i].append(rec)\n",
    "\n",
    "tree_matrix[0].insert(0, \"Pass\")\n",
    "tree_matrix[1].insert(0, \"Withdrawn\")\n",
    "tree_matrix[2].insert(0, \"Fail\")\n",
    "tree_matrix[3].insert(0, \"Distinction\")\n",
    "\n",
    "print()\n",
    "print(\"Decision Tree (Stratified 10 Fold Cross Validation Results)\")\n",
    "print(tabulate(tree_matrix, [\"Final Result\",\"Pass\",\"Withdrawn\",\"Fail\", \"Distinction\",\"total\", \"Recognition %\"]))\n"
   ]
  }
 ]
}